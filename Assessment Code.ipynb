{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP CODE ###\n",
    "\n",
    "#Code to set up two classification problems\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def genGaussianSamples(N, m, C):\n",
    "    A = np.linalg.cholesky(C)\n",
    "    U = np.random.randn(N,2)\n",
    "    return(U @ A.T + m)\n",
    "NClasses = 3\n",
    "# Priors\n",
    "#\n",
    "w = np.random.rand(NClasses)\n",
    "w = w / np.sum(w)\n",
    "N = 1000 # total data (Training = Test)\n",
    "NPrior = np.floor(w * N).astype(int)\n",
    "Scale = 10\n",
    "Means = Scale*np.random.rand(NClasses, 2)\n",
    "print(Means)\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "CovMatrices = np.zeros((NClasses,2,2))\n",
    "for j in range(NClasses):\n",
    "    CovMatrices[j,:,:] = make_spd_matrix(2)\n",
    "    AllData_train = list()\n",
    "for j in range(NClasses):\n",
    "    AllData_train.append(genGaussianSamples(NPrior[j], Means[j,:], CovMatrices[j,:,:]))\n",
    "    X_train = AllData_train[0]\n",
    "    y_train = np.ones((NPrior[0], 1))\n",
    "for j in range(NClasses-1):\n",
    "    Xj = genGaussianSamples(NPrior[j+1], Means[j+1,:], CovMatrices[j+1,:,:])\n",
    "    X_train = np.append(X_train, Xj, axis=0)\n",
    "    yj = (j+2)*np.ones((NPrior[j+1], 1))\n",
    "    y_train = np.append(y_train, yj)\n",
    "AllData_test = list()\n",
    "for j in range(NClasses):\n",
    "    AllData_test.append(genGaussianSamples(NPrior[j], Means[j,:], CovMatrices[j,:,:]))\n",
    "    X_test = AllData_test[0]\n",
    "    y_test = np.ones((NPrior[0], 1))\n",
    "for j in range(NClasses-1):\n",
    "    Xj = genGaussianSamples(NPrior[j+1], Means[j+1,:], CovMatrices[j+1,:,:])\n",
    "    X_test = np.append(X_test, Xj, axis=0)\n",
    "    yj = (j+2)*np.ones((NPrior[j+1], 1))\n",
    "    y_test = np.append(y_test, yj)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "for j in range(NClasses):\n",
    "    Xplt = AllData_train[j]\n",
    "    ax[0].scatter(Xplt[:,0], Xplt[:,1], s=3)\n",
    "    ax[0].grid(True)\n",
    "    ax[0].set_title(\"Training Data Distributions\")\n",
    "    ax[1].plot(y_train)\n",
    "    ax[1].set_title(\"Training Targets\")\n",
    "for j in range(NClasses):\n",
    "    Xplt = AllData_test[j]\n",
    "    ax[2].scatter(Xplt[:,0], Xplt[:,1], s=3)\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_title(\"Test Data Distributions\")\n",
    "plt.savefig('TrainTestDistcomp',tight = 'bbox_inches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ten cross validation and plotting boxplot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot_train = onehot_encoder.fit_transform(y_train.reshape(-1, 1))#Onehot encoding\n",
    "y_onehot_test = onehot_encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "clf = MLPClassifier(learning_rate_init = 0.1)\n",
    "gnb = GaussianNB()\n",
    "kf = KFold(n_splits = 10)\n",
    "\n",
    "X = np.concatenate((X_train,X_test))\n",
    "Y_onehot = np.concatenate((y_onehot_train,y_onehot_test))\n",
    "Y = np.concatenate((y_train, y_test))\n",
    "models = [gnb, clf]\n",
    "data = []\n",
    "names = [\"Naive Bayes\", \"MLP\"]\n",
    "df = pd.DataFrame(index=range(10 * len(models)))\n",
    "l = 0\n",
    "for model in models:\n",
    "    model_name = names[l]\n",
    "    if model_name == 'Naive Bayes':\n",
    "        acc = cross_val_score(model, X, Y, cv=10)#Storing the Ten-Fold crossvalidation accuracy\n",
    "    else:\n",
    "        acc = cross_val_score(model, X, Y_onehot, cv=10)\n",
    "    for fold_idx, accuracy in enumerate(acc):\n",
    "        data.append((model_name, fold_idx, accuracy))\n",
    "        df = pd.DataFrame(data, columns=['model_name_unigrams', 'fold_idx', 'accuracy'])\n",
    "    l = l + 1\n",
    "   \n",
    "   \n",
    "sns.boxplot(x='model_name_unigrams', y='accuracy', data=df)#Plotting the Boxplots\n",
    "plt.savefig('Boxplotgnbmlpcomp',bbox_inches = 'tight')\n",
    "plt.show()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = MLPClassifier(learning_rate_init = 0.1, alpha = 0.00001, random_state = 1) ##Code used to check convergence and to check test accuracy while changing alpha##\n",
    "# clf.fit(X_train,y_onehot_train)\n",
    "# fig, ax = plt.subplots(figsize=(6,4))\n",
    "# print(clf.score(X_test, y_onehot_test))\n",
    "# ax.set_title(\"Convergence\")\n",
    "# ax.set_xlabel(\"Iteration\", fontsize = 14)\n",
    "# ax.set_ylabel(\"Loss\", fontsize = 14)\n",
    "# ax.plot(clf.loss_curve_)\n",
    "# plt.savefig('Convergence01', bbox_inches = 'tight')\n",
    "\n",
    "\n",
    "#Code to plot the decision boundary\n",
    "h = 0.1\n",
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "print(x_min, x_max)\n",
    "print(y_min, y_max)\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "print(Z)\n",
    "Z = np.argmax(Z, axis=1)\n",
    "# print(Z)\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired) #PLotting the decision boundary\n",
    "plt.axis(\"tight\")\n",
    "colors = \"bry\"\n",
    "print(y_train)\n",
    "\n",
    "# Plotting the training points\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    idx = np.where(y_train == i+1)\n",
    "    plt.scatter(\n",
    "        X_train[idx, 0],\n",
    "        X_train[idx, 1],\n",
    "        c=color,\n",
    "        cmap=plt.cm.Paired,\n",
    "        edgecolor=\"black\",\n",
    "        s=20,\n",
    "    )\n",
    "plt.title(\"Decision boundary for MLP classifier\")\n",
    "plt.axis(\"tight\")\n",
    "plt.legend()\n",
    "plt.savefig(\"MLPDecisioncomp\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K_MEANS CODE ###\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import spatial\n",
    "\n",
    "def genGaussianSamples(N, m, C):\n",
    "    A = np.linalg.cholesky(C)\n",
    "    U = np.random.randn(N,2)\n",
    "    return(U @ A.T + m)\n",
    "# Define three means\n",
    "#\n",
    "Means = np.array([[0, 3], [3, 0], [4,4]])\n",
    "# Define three covariance matrices ensuring\n",
    "# they are positive definite\n",
    "#\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "CovMatrices = np.zeros((3,2,2))\n",
    "for j in range(3):\n",
    "    CovMatrices[j,:,:] = make_spd_matrix(2)\n",
    "# Priors\n",
    "#\n",
    "w = np.random.rand(3)\n",
    "w = w / np.sum(w)\n",
    "# How many data in each component (1000 in total)\n",
    "#\n",
    "nData = np.floor(w * 1000).astype(int)\n",
    "# Draw samples from each component\n",
    "#\n",
    "X0 = genGaussianSamples(nData[0], Means[0,:], CovMatrices[0,:,:])\n",
    "X1 = genGaussianSamples(nData[1], Means[1,:], CovMatrices[1,:,:])\n",
    "X2 = genGaussianSamples(nData[2], Means[2,:], CovMatrices[2,:,:])\n",
    "# Append into an array for the data we need\n",
    "#\n",
    "X = np.append(np.append(X0, X1, axis=0), X2, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the contours \n",
    "def gauss2D(x, m, C):\n",
    "    Ci = np.linalg.inv(C)\n",
    "    dC = np.linalg.det(Ci)\n",
    "    num = np.exp(-0.5 * np.dot((x-m).T,np.dot(Ci, (x-m))))\n",
    "    den = 2 * np.pi * np.sqrt(dC)\n",
    "    \n",
    "    return num/den\n",
    "\n",
    "def twoDGaussianPlot(nx, ny, m, C):\n",
    "    x = np.linspace(-8, 8, nx)\n",
    "    y = np.linspace(-8, 8, ny)\n",
    "    X, Y = np.meshgrid(x, y, indexing = 'ij')\n",
    "    Z = np.zeros([nx, ny])\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            xvec = np.array([X[i, j], Y[i, j]])\n",
    "            Z[i, j] = gauss2D(xvec, m ,C)\n",
    "    return X, Y, Z\n",
    "nx, ny = 50, 40\n",
    "m1 = Means[0,:]\n",
    "C1 = CovMatrices[0,:,:]\n",
    "Xp1, Yp1, Zp1 = twoDGaussianPlot(nx, ny, m1, C1)\n",
    "m2 = Means[1,:]\n",
    "C2 = CovMatrices[1,:,:]\n",
    "Xp2, Yp2, Zp2 = twoDGaussianPlot(nx, ny, m2, C2)\n",
    "m3 = Means[2,:]\n",
    "C3 = CovMatrices[2,:,:]\n",
    "Xp3, Yp3, Zp3 = twoDGaussianPlot(nx, ny, m3, C3)\n",
    "plt.figure(figsize = (15,10))\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "ax.scatter(X0[:,0], X0[:,1], c = \"c\", s=4)\n",
    "ax.scatter(X1[:,0], X1[:,1], c = \"m\", s=4)\n",
    "ax.scatter(X2[:,0], X2[:,1], c= \"r\", s=4)\n",
    "ax.set_xlim(-2, 8)\n",
    "ax.set_ylim(-2, 8)\n",
    "plt.contour(Xp1,Yp1,Zp1,5)\n",
    "plt.contour(Xp2,Yp2,Zp2,5)\n",
    "plt.contour(Xp3,Yp3,Zp3,5)\n",
    "plt.savefig('Contourplotofdata', tight = 'bbox_inches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means implementation\n",
    "K = 3\n",
    "datadist = np.zeros((len(X), K))\n",
    "def Kmeans(X, k, num_of_iter):\n",
    "        ix = np.random.choice(len(X), k, replace = False)\n",
    "        centroids = np.reshape(X[ix, :], (3,2)) #Randomly choosing centroids\n",
    "        for i in range(K):\n",
    "            datadist[:,i] = np.sqrt(np.sum((X - centroids[i,:])**2,axis = 1)) #Finding the euclidean distance between centroids and data points\n",
    "        Class = np.array(np.argmin(datadist, axis = 1)) #Assigning datapoints to classes based on distance to centroid\n",
    "       # p = np.argmin(spatial.distance.cdist(X, centroids, 'euclidean'), axis = 1)\n",
    "   \n",
    "        for _ in range(num_of_iter):\n",
    "            centroids = []\n",
    "            for j in range(k):\n",
    "                tmp_cent = X[Class == j].mean(axis = 0)\n",
    "                centroids.append(tmp_cent)\n",
    "            centroids = np.vstack(centroids)\n",
    "            plt.scatter(centroids[:,0],centroids[:,1],c = 'black',s= 20)#Plotting centroids obtained in each iteration\n",
    "            for i in range(K):\n",
    "                datadist[:,i] = np.sqrt(np.sum((X - centroids[i,:])**2,axis = 1)) #Finding the euclidean distance between centroids and data points\n",
    "                Class = np.array(np.argmin(datadist, axis = 1)) #Assigning datapoints to classes based on distance to centroid\n",
    "        plt.scatter(centroids[:,0], centroids[:,1], c='r',s = 50)# Plotting the centroid after iterations\n",
    "        print(centroids)\n",
    "        return Class   \n",
    "        #print(p)\n",
    "plt.figure(figsize = (8,8))\n",
    "Out = Kmeans(X, K ,300)\n",
    "\n",
    "\n",
    "for i in range(K):\n",
    "    plt.scatter(X[Out == i,0],X[Out == i,1],s = 20)\n",
    "plt.savefig('Centroidconvergence', tight = 'bbox_inches')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means using sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 0).fit(X)\n",
    "kmeans.labels_\n",
    "for i in range(K):\n",
    "    plt.scatter(X[kmeans.labels_ == i,0],X[kmeans.labels_ == i,1],s = 6)\n",
    "plt.savefig('SklearnKmeans', tight = 'bbox_inches')\n",
    "plt.show()\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Seeds dataset using K-Means\n",
    "\n",
    "from pandas import crosstab\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv(\"D:\\Msc Lab FML\\seeds_dataset.txt\",sep =r'\\t', header = None) #Reading the seeds dataset(UCI)\n",
    "df.head()\n",
    "scaler = StandardScaler() #Standardization\n",
    "labels = df.iloc[:,7] #Target values\n",
    "print(labels)\n",
    "X_train = df.drop(columns = [7])\n",
    "print(X_train)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "print(X_train)\n",
    "Output = KMeans(n_clusters = 3, random_state = 0).fit(X_train)\n",
    "Output.cluster_centers_\n",
    "print(Output.labels_)\n",
    "crosstab(np.array(labels), np.array(Output.labels_), rownames = ['label'], colnames = ['Cluster']) #Creating a table of labels and predicted clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
